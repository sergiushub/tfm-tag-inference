{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.platform import gfile\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "def run(cmd):\n",
    "    p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "    return p.stdout.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the constants to run this notebook.\n",
    "\n",
    "model_dir = '/tmp/imagenet'\n",
    "image_file = \"\"\n",
    "num_top_predictions = 5\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "\n",
    "IMG_URL = 'hdfs://localhost:9000/data/im6*.jpg'\n",
    "TAG_URL = 'hdfs://localhost:9000/data/tags/tags6*.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_img_rdd(x):\n",
    "    key = os.path.basename(x[0]).split('.')[0][2:]    \n",
    "    return (key,x[1])\n",
    "\n",
    "def clean_tags_rdd(x):\n",
    "    key = os.path.basename(x[0]).split('.')[0][4:]  \n",
    "    value = x[1].splitlines()\n",
    "    return (key,value)\n",
    "    \n",
    "def read_file_index():\n",
    "    im = sc.binaryFiles(IMG_URL).map(clean_img_rdd)\n",
    "    tg = sc.binaryFiles(TAG_URL).map(clean_tags_rdd)\n",
    "    \n",
    "    return im.join(tg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "    \"\"\"Download and extract model tar file.\"\"\"\n",
    "    from six.moves import urllib\n",
    "    dest_directory = model_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath2, _ = urllib.request.urlretrieve(DATA_URL, filepath)\n",
    "        print(\"filepath2\", filepath2)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "    else:\n",
    "        print('Data already downloaded:', filepath, os.stat(filepath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data already downloaded:', '/tmp/imagenet/inception-2015-12-05.tgz', posix.stat_result(st_mode=33188, st_ino=1975180, st_dev=66309, st_nlink=1, st_uid=0, st_gid=0, st_size=88931400, st_atime=1515549575, st_mtime=1515549584, st_ctime=1515549584))\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_extract()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = read_file_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lookup_path = os.path.join(model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "uid_lookup_path = os.path.join(model_dir, 'imagenet_synset_to_human_label_map.txt')\n",
    "\n",
    "def load_lookup():\n",
    "    \"\"\"Loads a human readable English name for each softmax node.\n",
    "    \n",
    "    Args:\n",
    "        label_lookup_path: string UID to integer node ID.\n",
    "        uid_lookup_path: string UID to human-readable string.\n",
    "\n",
    "    Returns:\n",
    "        dict from integer node ID to human-readable string.\n",
    "    \"\"\"\n",
    "    if not gfile.Exists(uid_lookup_path):\n",
    "        tf.logging.fatal('File does not exist %s', uid_lookup_path)\n",
    "    if not gfile.Exists(label_lookup_path):\n",
    "        tf.logging.fatal('File does not exist %s', label_lookup_path)\n",
    "\n",
    "    # Loads mapping from string UID to human-readable string\n",
    "    proto_as_ascii_lines = gfile.GFile(uid_lookup_path).readlines()\n",
    "    uid_to_human = {}\n",
    "    p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "    for line in proto_as_ascii_lines:\n",
    "        parsed_items = p.findall(line)\n",
    "        uid = parsed_items[0]\n",
    "        human_string = parsed_items[2]\n",
    "        uid_to_human[uid] = human_string\n",
    "\n",
    "    # Loads mapping from string UID to integer node ID.\n",
    "    node_id_to_uid = {}\n",
    "    proto_as_ascii = gfile.GFile(label_lookup_path).readlines()\n",
    "    for line in proto_as_ascii:\n",
    "        if line.startswith('  target_class:'):\n",
    "            target_class = int(line.split(': ')[1])\n",
    "        if line.startswith('  target_class_string:'):\n",
    "            target_class_string = line.split(': ')[1]\n",
    "            node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "    # Loads the final mapping of integer node ID to human-readable string\n",
    "    node_id_to_name = {}\n",
    "    for key, val in node_id_to_uid.items():\n",
    "        if val not in uid_to_human:\n",
    "            tf.logging.fatal('Failed to locate: %s', val)\n",
    "        name = uid_to_human[val]\n",
    "        node_id_to_name[key] = name\n",
    "\n",
    "    return node_id_to_name\n",
    "\n",
    "node_lookup = load_lookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_lookup_bc = sc.broadcast(node_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_dir, 'classify_image_graph_def.pb')\n",
    "with gfile.FastGFile(model_path, 'rb') as f:\n",
    "    model_data = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_bc = sc.broadcast(model_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_image(sess, img_id, image, tags, node_lookup):\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    predictions = sess.run(softmax_tensor,\n",
    "                            {'DecodeJpeg/contents:0': image})\n",
    "    predictions = np.squeeze(predictions)\n",
    "    top_k = predictions.argsort()[-num_top_predictions:][::-1]\n",
    "    scores = []\n",
    "    for node_id in top_k:\n",
    "        if node_id not in node_lookup:\n",
    "            human_string = ''\n",
    "        else:\n",
    "            human_string = node_lookup[node_id]\n",
    "        score = predictions[node_id]\n",
    "        #scores.append((human_string, score))\n",
    "        scores.append((node_id, score))\n",
    "    return (img_id, tags, scores)\n",
    "\n",
    "def apply_batch(image_entry):\n",
    "    img_id = image_entry[0]\n",
    "    image = image_entry[1][0]\n",
    "    tags = image_entry[1][1]\n",
    "    with tf.Graph().as_default() as g:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(model_data_bc.value)\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "        with tf.Session() as sess:\n",
    "            labelled = run_image(sess, img_id, image, tags, node_lookup_bc.value)\n",
    "            return labelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the images without tags -> x[1][1] are tags\n",
    "labelled_images = image_data.filter(lambda x: x[1][1]).map(apply_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'68',\n",
       "  ['velilladelavalduerna',\n",
       "   'le\\xc3\\xb3n',\n",
       "   'espa\\xc3\\xb1a',\n",
       "   'macromarvels',\n",
       "   'macromix',\n",
       "   'kartpostal',\n",
       "   'dragongold'],\n",
       "  [(328, 0.061709728),\n",
       "   (630, 0.050970137),\n",
       "   (647, 0.042599127),\n",
       "   (698, 0.042171303),\n",
       "   (743, 0.028100735)]),\n",
       " (u'62',\n",
       "  ['ala',\n",
       "   'anaheim',\n",
       "   'california',\n",
       "   'losangeles',\n",
       "   'la',\n",
       "   'ca',\n",
       "   'atheists',\n",
       "   'atheism',\n",
       "   'tee',\n",
       "   'funny',\n",
       "   'religion'],\n",
       "  [(837, 0.56478065),\n",
       "   (954, 0.080109961),\n",
       "   (961, 0.077204533),\n",
       "   (782, 0.0088803628),\n",
       "   (836, 0.0033093342)]),\n",
       " (u'64',\n",
       "  ['subway', 'motion', 'blur', 'explore'],\n",
       "  [(887, 0.18019484),\n",
       "   (681, 0.07882373),\n",
       "   (257, 0.066693686),\n",
       "   (904, 0.047006723),\n",
       "   (726, 0.04304871)]),\n",
       " (u'66',\n",
       "  ['porte',\n",
       "   'fen\\xc3\\xaatre',\n",
       "   'arras',\n",
       "   'window',\n",
       "   'door',\n",
       "   'decay',\n",
       "   '500x500',\n",
       "   'geo:lat=50291834',\n",
       "   'geo:lon=2781515',\n",
       "   'geotagged'],\n",
       "  [(990, 0.28706864),\n",
       "   (690, 0.16546968),\n",
       "   (686, 0.15377742),\n",
       "   (685, 0.026788786),\n",
       "   (702, 0.025027897)]),\n",
       " (u'67',\n",
       "  ['sigma15mmf28',\n",
       "   'beach',\n",
       "   'ivy',\n",
       "   'maren',\n",
       "   'bw',\n",
       "   'sky',\n",
       "   'sun',\n",
       "   'forloveofphotographydailyblog',\n",
       "   'lajolla',\n",
       "   'california'],\n",
       "  [(367, 0.77648342),\n",
       "   (719, 0.13742603),\n",
       "   (363, 0.035762973),\n",
       "   (733, 0.0077084457),\n",
       "   (248, 0.0059942901)]),\n",
       " (u'61',\n",
       "  ['350d',\n",
       "   'dacha',\n",
       "   'houseslippers',\n",
       "   'landscape',\n",
       "   'sigma1770f2845',\n",
       "   'uyma',\n",
       "   'slippers',\n",
       "   'flickr',\n",
       "   'explore',\n",
       "   'interestingness'],\n",
       "  [(972, 0.34671441),\n",
       "   (751, 0.3281346),\n",
       "   (973, 0.10353078),\n",
       "   (769, 0.047290258),\n",
       "   (709, 0.015384815)]),\n",
       " (u'63',\n",
       "  ['macro', 'nature', 'epistrophe', 'balteata', 'flickrlovers'],\n",
       "  [(629, 0.87511724),\n",
       "   (745, 0.023541592),\n",
       "   (630, 0.017635932),\n",
       "   (684, 0.0034218188),\n",
       "   (639, 0.0019627975)]),\n",
       " (u'65',\n",
       "  ['zoo',\n",
       "   'zoologico',\n",
       "   'mayaguez',\n",
       "   'mariposario',\n",
       "   'monarca',\n",
       "   'butterfly',\n",
       "   'mariposa',\n",
       "   'vivid',\n",
       "   'tones',\n",
       "   'natural'],\n",
       "  [(643, 0.92171556),\n",
       "   (238, 0.0023500393),\n",
       "   (515, 0.0014663932),\n",
       "   (908, 0.001107342),\n",
       "   (319, 0.00069091161)]),\n",
       " (u'6',\n",
       "  ['iceland',\n",
       "   'icelandic',\n",
       "   'reykjavik',\n",
       "   'ice',\n",
       "   'sky',\n",
       "   'pink',\n",
       "   'white',\n",
       "   'blue',\n",
       "   'clouds',\n",
       "   'lake',\n",
       "   'museum'],\n",
       "  [(596, 0.86975491),\n",
       "   (680, 0.027495915),\n",
       "   (795, 0.015630763),\n",
       "   (733, 0.013062296),\n",
       "   (834, 0.0098934276)]),\n",
       " (u'69',\n",
       "  ['white',\n",
       "   'panasonic',\n",
       "   'unaciertamirada',\n",
       "   'flickr',\n",
       "   'bw',\n",
       "   'blackwhite',\n",
       "   'blackandwhite',\n",
       "   'black',\n",
       "   'flickrestrellas',\n",
       "   'antetnas',\n",
       "   'sky',\n",
       "   'cielo',\n",
       "   'urban',\n",
       "   'spain',\n",
       "   'lumix',\n",
       "   'lugares',\n",
       "   'leica',\n",
       "   'europe',\n",
       "   'espa\\xc3\\xb1a',\n",
       "   'ciudades',\n",
       "   'city',\n",
       "   'skyscraper',\n",
       "   'panasonicdmcfz50',\n",
       "   'building'],\n",
       "  [(245, 0.41936967),\n",
       "   (923, 0.088324949),\n",
       "   (221, 0.060100295),\n",
       "   (248, 0.029145522),\n",
       "   (715, 0.010718866)])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_labelled_images = labelled_images.collect()\n",
    "\n",
    "local_labelled_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.0135332792997,-0.011096050078,0.0506678894162]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.0376478566655,0.0210807355387,0.0403019455927]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.0177810560912,-0.0559235086665,-0.0178805116564]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c, d]   |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c d\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=4, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
